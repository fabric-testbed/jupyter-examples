{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploying a Simple Kubernetes Cluster on FABRIC\n",
    "\n",
    "This notebook demonstrates how to deploy a basic Kubernetes (K8s) cluster on the [FABRIC](https://fabric-testbed.net) testbed using the FABlib API.  \n",
    "The cluster is built from scratch by provisioning virtual machines (VMs), configuring dataplane networking, installing Kubernetes components, and setting up the Flannel CNI (Container Network Interface) plugin.\n",
    "\n",
    "The main steps include:\n",
    "\n",
    "- Verifying connectivity to all sites\n",
    "- Creating a slice and adding controller and worker nodes\n",
    "- Connecting nodes over a Layer 3 FabNetv4 network (easily extendable to Layer 2)\n",
    "- Uploading configuration and helper scripts to all nodes\n",
    "- Installing K8s prerequisites and components (`kubeadm`, `kubelet`, `kubectl`)\n",
    "- Bootstrapping the controller node and configuring it to use the dataplane network\n",
    "- Deploying the Flannel CNI and patching it for compatibility with IPv6 management networks\n",
    "- Joining worker nodes to the cluster\n",
    "- Deploying a test application (nginx) and validating the cluster\n",
    "\n",
    "This setup enables users to launch and manage a minimal, functional K8s cluster suitable for experimentation and learning on FABRIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FABRIC Library Initialization\n",
    "\n",
    "We begin by importing and configuring the FABRIC library to manage slice and node provisioning within the testbed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "fablib = fablib_manager()\n",
    "fablib.show_config();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Slice and Nodes\n",
    "\n",
    "This section provisions a new FABRIC slice that includes:\n",
    "\n",
    "- A **controller node**\n",
    "- One or more **worker nodes**\n",
    "\n",
    "These nodes are distributed across the desired FABRIC sites. During provisioning, we specify:\n",
    "- The **disk size**\n",
    "- The **OS image**\n",
    "- The **network configuration**\n",
    "\n",
    "By default, all nodes are connected to the **Layer 3 FABNetv4 network**, enabling routed connectivity across sites.\n",
    "\n",
    "> **Note:** This setup can be easily extended to use a **Layer 2 network** by modifying the network attachment configuration during node creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_name = 'MySlice-k8s'\n",
    "[site1, site2] = fablib.get_random_sites(count=2)\n",
    "print(f\"Sites: {site1} {site2}\")\n",
    "ctrlr_name  = \"ctrlr\"\n",
    "node1_name = 'node1'\n",
    "node2_name = 'node2'\n",
    "image = 'default_ubuntu_22'\n",
    "ctrlr_nic_name = 'NIC1'\n",
    "node1_nic_name = 'NIC1'\n",
    "node2_nic_name = 'NIC1'\n",
    "net1_name = \"net1\"\n",
    "net2_name = \"net2\"\n",
    "net3_name = \"net3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #Create Slice\n",
    "    slice = fablib.new_slice(slice_name)\n",
    "\n",
    "    # Networks\n",
    "    net1 = slice.add_l3network(name=net1_name, type='IPv4')\n",
    "    net2 = slice.add_l3network(name=net2_name, type='IPv4')\n",
    "\n",
    "    # Add node\n",
    "    ctrlr = slice.add_node(name=ctrlr_name, site=site1, image=image)\n",
    "    iface1 = ctrlr.add_component(model='NIC_Basic', name=ctrlr_nic_name).get_interfaces()[0]\n",
    "    iface1.set_mode('auto')\n",
    "    net1.add_interface(iface1)\n",
    "    ctrlr.add_route(subnet=fablib.FABNETV4_SUBNET, next_hop=net1.get_gateway())\n",
    "\n",
    "    # Add node\n",
    "    node1 = slice.add_node(name=node1_name, site=site2, image=image)\n",
    "    iface2 = node1.add_component(model='NIC_Basic', name=node1_nic_name).get_interfaces()[0]\n",
    "    iface2.set_mode('auto')\n",
    "    net2.add_interface(iface2)\n",
    "    node1.add_route(subnet=fablib.FABNETV4_SUBNET, next_hop=net2.get_gateway())\n",
    "    \n",
    "    # Add node\n",
    "    node2 = slice.add_node(name=node2_name, site=site2, image=image)\n",
    "    iface3 = node2.add_component(model='NIC_Basic', name=node2_nic_name).get_interfaces()[0]\n",
    "    iface3.set_mode('auto')\n",
    "    net2.add_interface(iface3)\n",
    "    node2.add_route(subnet=fablib.FABNETV4_SUBNET, next_hop=net2.get_gateway())\n",
    "    \n",
    "    #Submit Slice Request\n",
    "    slice_id = slice.submit()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Connectivity\n",
    "\n",
    "Before proceeding, verify that all the nodes in the slice are reachable via SSH. This ensures that the provisioning and network configuration were successful.\n",
    "\n",
    "Typical checks include:\n",
    "- Verifying that each node responds to basic `hostname` or `uptime` commands\n",
    "- Ensuring network connectivity between nodes (e.g., via `ping`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = fablib.get_slice(slice_name)\n",
    "ctrlr = slice.get_node(name=ctrlr_name)        \n",
    "node1 = slice.get_node(name=node1_name)        \n",
    "node2 = slice.get_node(name=node2_name)           \n",
    "\n",
    "ctrlr_iface = ctrlr.get_interface(network_name=net1_name)\n",
    "ctrlr_addr = ctrlr.get_interface(network_name=net1_name).get_ip_addr()\n",
    "node1_addr = node1.get_interface(network_name=net2_name).get_ip_addr()\n",
    "node2_addr = node2.get_interface(network_name=net2_name).get_ip_addr()\n",
    "\n",
    "stdout, stderr = ctrlr.execute(f'ping -c 5 {node1_addr}')\n",
    "stdout, stderr = ctrlr.execute(f'ping -c 5 {node2_addr}')\n",
    "stdout, stderr = node1.execute(f'ping -c 5 {node2_addr}')\n",
    "\n",
    "\n",
    "node_ips = {\n",
    "    node1.get_name() : node1_addr,\n",
    "    node2.get_name() : node2_addr\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure SSH Key-Based Access\n",
    "\n",
    "Generate SSH key pairs for both the `ubuntu` and `root` users.  \n",
    "Distribute the corresponding public keys to all nodes by appending them to the appropriate `authorized_keys` files, enabling passwordless SSH access between nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in slice.get_nodes():\n",
    "    n.execute('ssh-keygen -t rsa -N \"\" -f /home/ubuntu/.ssh/id_rsa', quiet=True)\n",
    "    n.execute('sudo ssh-keygen -t rsa -N \"\" -f /root/.ssh/id_rsa', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {}\n",
    "# Step 1: Collect public keys from each node\n",
    "for n in slice.get_nodes():\n",
    "    ubuntu_key, _ = n.execute(\"cat /home/ubuntu/.ssh/id_rsa.pub\", quiet=True)\n",
    "    root_key, _ = n.execute(\"sudo cat /root/.ssh/id_rsa.pub\", quiet=True)\n",
    "    keys[n.get_name()] = {\n",
    "        \"ubuntu\": ubuntu_key.strip(),\n",
    "        \"root\": root_key.strip()\n",
    "    }\n",
    "\n",
    "# Step 2: Distribute public keys to all other nodes\n",
    "for n in slice.get_nodes():\n",
    "    for other_node_name, node_keys in keys.items():\n",
    "        if other_node_name == n.get_name():\n",
    "            continue\n",
    "        n.execute(f'echo \"{node_keys[\"ubuntu\"]}\" >> /home/ubuntu/.ssh/authorized_keys')\n",
    "        n.execute(f'sudo sh -c \\'echo \"{node_keys[\"root\"]}\" >> /root/.ssh/authorized_keys\\'')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup `/etc/hosts`\n",
    "\n",
    "To enable seamless hostname-based communication over the **data plane network** (FABNetv4 in this example), this section configures the `/etc/hosts` file on each node.\n",
    "\n",
    "Each nodeâ€™s `/etc/hosts` entry includes:\n",
    "- The **data plane IP address** of every other node\n",
    "- The **corresponding hostname** (e.g., `ctrlr`, `node1`, etc.)\n",
    "\n",
    "This setup allows nodes to resolve and connect to each other using hostnames over the FABNetv4 network.\n",
    "\n",
    "> This is particularly useful when deploying Kubernetes or other distributed services where consistent host resolution is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in slice.get_nodes():\n",
    "    for node_name, ip in node_ips.items():\n",
    "        if n.get_name() == node_name:\n",
    "            n.execute(f'sudo sh -c \\'echo \"{ctrlr_addr} {ctrlr.get_name()}\" >> /etc/hosts\\'')\n",
    "            continue\n",
    "        n.execute(f'sudo sh -c \\'echo \"{ip} {node_name}\" >> /etc/hosts\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Scripts\n",
    "\n",
    "This step uploads the necessary helper scripts to all nodes in the slice (both controller and workers).\n",
    "\n",
    "These scripts typically include:\n",
    "- Kubernetes installation and initialization scripts\n",
    "- Flannel CNI configuration and patch scripts\n",
    "- Utility or setup helpers (e.g., interface detection, health checks)\n",
    "\n",
    "> Automating script distribution ensures consistency across nodes and simplifies the deployment process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in slice.get_nodes():\n",
    "    n.upload_directory(local_directory_path=\"./node_tools\", remote_directory_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install K8s Pre-requisites\n",
    "\n",
    "This step installs the required software components necessary for setting up Kubernetes on each node, such as:\n",
    "\n",
    "- `containerd`\n",
    "- Additional utilities (e.g., `curl`, `apt-transport-https`)\n",
    "\n",
    "After installation, the nodes are rebooted to ensure changes take effect cleanly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in slice.get_nodes():\n",
    "    n.execute(\"sudo ./node_tools/k8s_pre_install.sh\", quiet=True, output_file=f\"logs/{n.get_name()}_pre_install.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for the Servers to be Back Up and Re-apply Network Configuration\n",
    "\n",
    "After the reboot, the block waits for each node to become reachable via SSH.\n",
    "\n",
    "Once the nodes are accessible, the network configuration (e.g., default routes, DNS settings) is re-applied to ensure proper connectivity on the data plane (FabNetv4 in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice.wait_ssh()\n",
    "for n in slice.get_nodes():\n",
    "    n.config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install K8s Components\n",
    "\n",
    "Install the core Kubernetes components on each node:\n",
    "\n",
    "- `kubelet`: The primary \"node agent\" that runs on each node.\n",
    "- `kubeadm`: A tool to bootstrap the cluster.\n",
    "- `kubectl`: The command-line tool for interacting with the Kubernetes API.\n",
    "\n",
    "These components are installed in preparation for initializing the control plane and joining worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in slice.get_nodes():\n",
    "    n.execute(\"sudo ./node_tools/k8s_install.sh\", quiet=True, output_file=f\"logs/{n.get_name()}_install.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Controller\n",
    "\n",
    "This step initializes the Kubernetes control plane on the designated controller node using `kubeadm init`.\n",
    "\n",
    "Key configuration options include:\n",
    "\n",
    "- `--control-plane-endpoint`: The advertised IP address for the control plane.\n",
    "- `--apiserver-advertise-address`: The IP address the API server binds to.\n",
    "- `--pod-network-cidr`: The CIDR for the pod network.\n",
    "\n",
    "After initialization, the Kubernetes admin config (`admin.conf`) is copied to the user's home directory to enable use of `kubectl`.\n",
    "\n",
    "### Kubelet Config\n",
    "\n",
    "Configure the kubelet to prefer the dataplane network (FabNetv4 in this example) for node IP assignment instead of the default management network.\n",
    "\n",
    "This is done by setting the `--node-ip` flag in the kubelet systemd configuration to the dataplane IP address. This ensures that all Kubernetes components use the correct interface for intra-cluster communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ctrlr.execute(f\"sudo node_tools/k8s_kubelet.sh {ctrlr_addr}\", output_file=f\"logs/{ctrlr.get_name()}_kubelet.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ctrlr.execute(f\"node_tools/k8s_init_control_plane.sh {ctrlr_addr}\", output_file=f\"logs/{ctrlr.get_name()}_init.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Cluster State\n",
    "\n",
    "At this stage, the control plane should be up and running. Use `kubectl get nodes -o wide` and `kubectl get pods -A` to verify the state of the cluster.\n",
    "\n",
    "**Note:** It's expected that the DNS pods (e.g., CoreDNS) may show a `CrashLoopBackOff` or `Pending` status until the network plugin (e.g., Flannel or Calico) is deployed and functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ctrlr.execute(\"node_tools/k8s_status.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Network Plugin\n",
    "\n",
    "Deploy the Flannel CNI plugin to enable pod networking:\n",
    "\n",
    "```\n",
    "kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml\n",
    "```\n",
    "In environments where the management network uses IPv6, Flannel may fail to reach the Kubernetes API server via the default route.\n",
    "To address this, patch the Flannel DaemonSet to:\n",
    "\n",
    "Explicitly specify the data plane interface (e.g., --iface-can-reach=<API_SERVER_IP> or --iface-can-reach=<API_SERVER_IP>)\n",
    "\n",
    "Set the following environment variables:\n",
    "\n",
    "KUBERNETES_SERVICE_HOST: Set to the IPv4 API server address\n",
    "\n",
    "KUBERNETES_SERVICE_PORT: Set to 6443\n",
    "\n",
    "This ensures that Flannel uses the correct interface and successfully connects to the Kubernetes control plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ctrlr.execute(f\"node_tools/k8s_flannel.sh {ctrlr_addr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Cluster State\n",
    "\n",
    "After the network plugin (Flannel) has been deployed and patched, verify the health of the cluster.\n",
    "\n",
    "Run the following commands:\n",
    "\n",
    "```\n",
    "kubectl get nodes -o wide\n",
    "kubectl get pods -A -o wide\n",
    "```\n",
    "\n",
    "All system pods, including CoreDNS, should now be in a Running state.\n",
    "The cluster is considered healthy once all components are operational and no pods are stuck in Pending, CrashLoopBackOff, or Error states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ctrlr.execute(\"node_tools/k8s_status.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Nodes\n",
    "\n",
    "To join additional worker nodes to the Kubernetes cluster:\n",
    "\n",
    "1. **Retrieve the Join Command from the Controller Node:**\n",
    "\n",
    "   On the controller, run:\n",
    "\n",
    "   ```\n",
    "   kubeadm token create --print-join-command\n",
    "   ```\n",
    "   This will output a command like:\n",
    "   ```\n",
    "   kubeadm join <controller-ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>\n",
    "   ```\n",
    "---\n",
    "2. **Run the Join Command on Each Worker Node:**\n",
    "   ```\n",
    "   sudo kubeadm join 10.128.3.2:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "   ```\n",
    "---\n",
    "3. **Verify Node Addition:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_command, stderr = ctrlr.execute(\"kubeadm token create --print-join-command\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in slice.get_nodes():\n",
    "    if n.get_name() == ctrlr.get_name():\n",
    "        continue\n",
    "    n.execute(f\"sudo {join_command}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Cluster State\n",
    "\n",
    "With all nodes added, verify that the cluster is fully operational:\n",
    "\n",
    "1. **Check Node Readiness:**\n",
    "\n",
    "   ```bash\n",
    "   kubectl get nodes -o wide\n",
    "   ```\n",
    "   All nodes should show STATUS = Ready. It may take a few minutes after joining for nodes to transition to the ready state.\n",
    "---\n",
    "2. **Check Pod Health:**\n",
    "   ```\n",
    "   kubectl get pods -A -o wide\n",
    "   ```\n",
    "   All system and network plugin pods should be in the `Running` state.\n",
    "\n",
    "A healthy cluster with multiple nodes in `Ready` state confirms successful setup and node integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ctrlr.execute(\"node_tools/k8s_status.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy an Application\n",
    "\n",
    "Deploy a simple `nginx` web application to validate the Kubernetes cluster setup.\n",
    "\n",
    "### 1. Create the Deployment\n",
    "\n",
    "```\n",
    "kubectl apply -f node_tools/pod.yml\n",
    "````\n",
    "\n",
    "### 2. Expose the Deployment via NodePort\n",
    "\n",
    "```\n",
    "kubectl apply -f node_tools/service_nodeport.yml\n",
    "```\n",
    "\n",
    "This creates a service that maps a random port on each node to port 80 of the `nginx` pod.\n",
    "\n",
    "### 3. Wait for the Service and Pod to be Ready\n",
    "\n",
    "Check the service and pod status:\n",
    "\n",
    "```\n",
    "node_tools/k8s_status.sh\n",
    "```\n",
    "\n",
    "Note:\n",
    "\n",
    "* The **NodePort** assigned (e.g., `300080`)\n",
    "* The **node IP** where the `nginx` pod is running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ctrlr.execute(\"kubectl apply -f node_tools/pod.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ctrlr.execute(\"kubectl apply -f node_tools/service_nodeport.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ctrlr.execute(\"node_tools/k8s_status.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the SSH Tunnel\n",
    "\n",
    "- Create SSH Tunnel Configuration `fabric_ssh_tunnel_tools.zip`\n",
    "- Download your custom `fabric_ssh_tunnel_tools.zip` tarball from the `fabric_config` folder.  \n",
    "- Untar the tarball and put the resulting folder (`fabric_ssh_tunnel_tools`) somewhere you can access it from the command line.\n",
    "- Open a terminal window. (Windows: use `powershell`) \n",
    "- Use `cd` to navigate to the `fabric_ssh_tunnel_tools` folder.\n",
    "- In your terminal, run the command that results from running the following cell (leave the terminal window open)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fablib.create_ssh_tunnel_config(overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Port on your local machine that you want to map the File Browser to.\n",
    "local_port='30080'\n",
    "# Local interface to map the File Browser to (can be `localhost`)\n",
    "local_host='127.0.0.1'\n",
    "\n",
    "# Port on the node used by the File Browser Service\n",
    "target_port='30080'\n",
    "\n",
    "# Username/node on FABRIC\n",
    "target_host=f'{ctrlr.get_username()}@{ctrlr.get_management_ip()}'\n",
    "\n",
    "print(f'ssh  -L {local_host}:{local_port}:127.0.0.1:{target_port} -i {os.path.basename(fablib.get_default_slice_public_key_file())[:-4]} -F ssh_config {target_host}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Nginx Server\n",
    "\n",
    "The Nginx service running on K8s cluster is now mapped to 127.0.0.1:30080 on your local machine. You can open a browser and navigate to the following address (or just click the link): \n",
    "\n",
    "[http://127.0.0.1:30080](http://127.0.0.1:30080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Delete Slice\n",
    "\n",
    "Please delete your slicd when you are done with your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice = fablib.get_slice(slice_name)\n",
    "#slice.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

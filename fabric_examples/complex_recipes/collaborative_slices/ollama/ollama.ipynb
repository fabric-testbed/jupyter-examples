{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Slice, Launch ollama with LLM model\n",
    "\n",
    "This notebook provisions a slice on a single site, deploying one node equipped with a GPU and connected to a NIC_Basic via the FABNetv4 service. \n",
    "\n",
    "On this node, we install and configure Ollama to use the *deepseek-r1:7b* model and set up Open-WebUI on the VM. \n",
    "\n",
    "By establishing SSH tunnels, you can access Open-WebUI to submit queries through the web interface or interact with the LLM via the API. \n",
    "\n",
    "Additionally, nodes in other FABRIC slices connected to FABNetv4 can send queries to this LLM through the API over the FabNetv4 network. \n",
    "\n",
    "While this example utilizes FabNetv4, it can be adapted to work with the FabNetv6 service as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the FABlib Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipaddress import ip_address, IPv4Address, IPv6Address, IPv4Network, IPv6Network\n",
    "import ipaddress\n",
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "\n",
    "fablib = fablib_manager()\n",
    "\n",
    "fablib.show_config();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the Experiment Slice\n",
    "\n",
    "This section identifies a FABRIC site with an available GPU and sufficient CPU, RAM, and disk resources. Once a suitable site is found, a node is added with a GPU and a basic NIC, and it is connected to the FABNetv4 network to enable communication with other slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_slice_name = 'Ollama-slice'\n",
    "\n",
    "ollama_node_name ='ollama_node'\n",
    "\n",
    "network_name='net1'\n",
    "nic_name = 'nic1'\n",
    "model_name = 'NIC_Basic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a Site  \n",
    "Choose a GPU model and search for a site that offers the specified GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cores = 16\n",
    "min_ram_gb = 32\n",
    "min_disk_gb = 100\n",
    "min_gpu_any = 0       # >0 means at least one GPU of any model for the initial filter\n",
    "min_gpu_for_pick = 1  # >1 means at least two for the random pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "fields = ['name', 'state', 'cores_available', 'ram_available', 'disk_available']\n",
    "gpu_models = [\"GPU_RTX6000\", \"GPU_Tesla_T4\", \"GPU_A30\", \"GPU_A40\"]\n",
    "gpu_fields = [f\"{m.split('_', 1)[1].lower()}_available\" for m in gpu_models]\n",
    "fields += [f for f in gpu_fields if f not in fields]\n",
    "\n",
    "# If empty -> do not filter by name\n",
    "sites_like: list[str] = []   # e.g., ['BRIST', 'TOKY'] or [] to disable\n",
    "avoid_like: list[str] = [\"TACC\", \"GATECH\", \"GPN\"]   # e.g., ['BRIST', 'TOKY'] or [] to disable\n",
    "min_cores = 4\n",
    "min_ram_gb = 16\n",
    "min_disk_gb = 200\n",
    "min_gpu_any = 0       # >0 means at least one GPU of any model for the initial filter\n",
    "min_gpu_for_pick = 1  # >1 means at least two for the random pick\n",
    "\n",
    "def filter_function(row: dict) -> bool:\n",
    "    # Name filter: only apply if sites_like is non-empty\n",
    "    if sites_like:\n",
    "        name = (row.get('name') or '')\n",
    "        name_ok = any(tok.lower() in name.lower() for tok in sites_like)\n",
    "    else:\n",
    "        name_ok = True\n",
    "\n",
    "    res_ok = (\n",
    "        row.get('cores_available', 0) > min_cores and\n",
    "        row.get('ram_available', 0) > min_ram_gb and\n",
    "        row.get('disk_available', 0) > min_disk_gb and\n",
    "        row.get('state') == 'Active'\n",
    "    )\n",
    "    any_gpu_ok = any(row.get(gf, 0) > min_gpu_any for gf in gpu_fields)\n",
    "\n",
    "    return name_ok and res_ok and any_gpu_ok\n",
    "\n",
    "styled_or_df = fablib.list_hosts(fields=fields, pretty_names=False, avoid=avoid_like, filter_function=filter_function)\n",
    "\n",
    "# Normalize Styler/DataFrame/list-of-dicts -> DataFrame\n",
    "if isinstance(styled_or_df, pd.io.formats.style.Styler):\n",
    "    df = styled_or_df.data\n",
    "elif isinstance(styled_or_df, pd.DataFrame):\n",
    "    df = styled_or_df\n",
    "else:\n",
    "    df = pd.DataFrame(styled_or_df or [])\n",
    "\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"No hosts matched the filter criteria.\")\n",
    "\n",
    "# Random pick where any GPU count > 1\n",
    "model_map = dict(zip(gpu_fields, gpu_models))\n",
    "long = (\n",
    "    df.reset_index()[[\"index\"] + gpu_fields]\n",
    "      .melt(id_vars=\"index\", var_name=\"gpu_field\", value_name=\"count\")\n",
    ")\n",
    "eligible = long[long[\"count\"] > min_gpu_for_pick]\n",
    "if eligible.empty:\n",
    "    raise RuntimeError(\"No site has any GPU model with count > 1.\")\n",
    "\n",
    "pick = eligible.sample(1).iloc[0]\n",
    "host_row = df.loc[pick[\"index\"]]\n",
    "picked_gpu_model = model_map[pick[\"gpu_field\"]]\n",
    "\n",
    "print(\n",
    "    f\"Chosen Host: {host_row.get('name', '<unknown>')} | \"\n",
    "    f\"GPU: {picked_gpu_model} | Available: {int(pick['count'])}\"\n",
    ")\n",
    "\n",
    "if \"GPU_Tesla_T4\" == picked_gpu_model:\n",
    "    picked_gpu_model = \"GPU_TeslaT4\"\n",
    "\n",
    "picked_host = host_row.get('name')\n",
    "picked_site = picked_host.split('-', 1)[0].upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the Slice  \n",
    "\n",
    "Users can specify alternative models such as:  \n",
    "\n",
    "`llama2-7b`, `mistral-7b`, `gemma-7b`, `deepseek-r1:67b`, `phi-2`, `gpt-neo-2.7b`  \n",
    "\n",
    "For more available models, visit: [Ollama Model Search](https://ollama.com/search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_llm_model = \"deepseek-r1:7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Slice\n",
    "ollama_slice = fablib.new_slice(name=ollama_slice_name)\n",
    "\n",
    "net1 = ollama_slice.add_l3network(name=network_name)\n",
    "\n",
    "ollama_node = ollama_slice.add_node(name=ollama_node_name, cores=min_cores, ram=min_ram_gb, host=picked_host,\n",
    "                                    disk=min_disk_gb, site=picked_site, image='default_ubuntu_22')\n",
    "\n",
    "ollama_node.add_component(model=picked_gpu_model, name='gpu1')\n",
    "\n",
    "\n",
    "iface1 = ollama_node.add_component(model=model_name, name=nic_name).get_interfaces()[0]\n",
    "iface1.set_mode('auto')\n",
    "net1.add_interface(iface1)\n",
    "\n",
    "ollama_node.add_post_boot_upload_directory('ollama_tools','.')\n",
    "ollama_node.add_post_boot_upload_directory('node_tools','.')\n",
    "ollama_node.add_post_boot_execute('node_tools/enable_docker.sh {{ _self_.image }} ')\n",
    "ollama_node.add_post_boot_execute('node_tools/dependencies.sh {{ _self_.image }} ')\n",
    "ollama_node.add_post_boot_execute(f'cd ollama_tools && cp env.template .env && sed -i \"s/^MODEL_NAME=.*/MODEL_NAME={default_llm_model}/\" .env && docker compose up -d')\n",
    "\n",
    "ollama_slice.submit();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query LLM via API  \n",
    "\n",
    "This section demonstrates how to interact with the LLM using a Python API. We upload the `query.py` script to the `ollamanode` and execute it to send queries to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_slice=fablib.get_slice(ollama_slice_name)\n",
    "ollama_node = ollama_slice.get_node(ollama_node_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm Container Status  \n",
    "\n",
    "The containers may take a few minutes to start. Please verify that they are running before sending any queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(\"docker ps -a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(\"docker logs ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stdout, stderr = ollama_node.execute(\"docker logs open-webui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(f'python3 ollama_tools/query.py --model {default_llm_model} --prompt \"Hello World\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Access to Ollama Node Across FABRIC  \n",
    "\n",
    "Configure the `ollamanode` to be accessible from any VM running across FABRIC on FabNetV4 by setting up the necessary routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_fabnet_network = ollama_slice.get_network(network_name)\n",
    "\n",
    "ollama_node.add_route(subnet=fablib.FABNETV4_SUBNET, \n",
    "                      next_hop=ollama_fabnet_network.get_gateway())\n",
    "\n",
    "ollama_node.config_routes()\n",
    "\n",
    "stdout, stderr = ollama_node.execute(\"sudo ip route list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the FabNet IP Address  \n",
    "Display the FabNet IP address of the Ollama node for sharing with other slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_fabnet_ip_addr = ollama_node.get_interface(network_name=network_name).get_ip_addr()\n",
    "\n",
    "print(f\"Ollama is accessible from other slices at: {ollama_fabnet_ip_addr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Ollama\n",
    "\n",
    "Users can interact with the LLM through the REST API, the command-line interface, or the Open WebUI.\n",
    "\n",
    "### REST Examples\n",
    "\n",
    "The `query.py` script demonstrates how to query the LLM over the REST interface. Although Ollama can run on a remote host, the example below targets the local instance by passing `--host localhost`. Users may also specify a different `--host` and `--port` as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(f\"python3 ollama_tools/query.py --model {default_llm_model} --prompt 'Tell me about National Science Foundation' --host localhost --port 11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(f\"python3 ollama_tools/query.py --model {default_llm_model} --prompt 'Tell me about NVIDIA BlueField DPUs' --host localhost --port 11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLI Examples\n",
    "\n",
    "SSH into the `ollama_node` using the command provided above.\n",
    "To view available models, run:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama list\n",
    "```\n",
    "\n",
    "To start a model and interact with it:\n",
    "\n",
    "```bash\n",
    "docker exec -it ollama ollama run deepseek-r1:7b\n",
    "```\n",
    "\n",
    "This will open an interactive prompt where you can type questions directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Web UI\n",
    "\n",
    "To access the Open Web UI from your laptop, you’ll need to create an SSH tunnel.\n",
    "Follow the steps below to complete the setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the SSH Tunnel\n",
    "\n",
    "- Create SSH Tunnel Configuration `fabric_ssh_tunnel_tools.zip`\n",
    "- Download your custom `fabric_ssh_tunnel_tools.zip` tarball from the `fabric_config` folder.  \n",
    "- Untar the tarball and put the resulting folder (`fabric_ssh_tunnel_tools`) somewhere you can access it from the command line.\n",
    "- Open a terminal window. (Windows: use `powershell`) \n",
    "- Use `cd` to navigate to the `fabric_ssh_tunnel_tools` folder.\n",
    "- In your terminal, run the command that results from running the following cell (leave the terminal window open)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fablib.create_ssh_tunnel_config(overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch Open Web UI\n",
    "\n",
    "To access the Open Web UI running on the ollama node, create an SSH tunnel from your local machine using the command generated by the next cell:\n",
    "\n",
    "```bash\n",
    "ssh -L 8080:<manager-ip>:8080 -i <private_key> -F <ssh_config> <your-username>@<manager-host>\n",
    "```\n",
    "\n",
    "Replace `<manager-ip>` and `<manager-host>` with the actual IP address and hostname of the Ceph manager VM.\n",
    "\n",
    "Then, open your browser and navigate to:\n",
    "\n",
    "\n",
    "http://localhost:8080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Port on your local machine that you want to map the File Browser to.\n",
    "local_port='8080'\n",
    "# Local interface to map the File Browser to (can be `localhost`)\n",
    "local_host='127.0.0.1'\n",
    "\n",
    "# Port on the node used by the File Browser Service\n",
    "target_port='8080'\n",
    "\n",
    "# Username/node on FABRIC\n",
    "target_host=f'{ollama_node.get_username()}@{ollama_node.get_management_ip()}'\n",
    "\n",
    "print(\"Use `cd` to navigate into the `fabric_ssh_tunnel_tools` folder.\")\n",
    "print(\"In your terminal, run the SSH tunnel command\")\n",
    "print()\n",
    "print(f'ssh  -L {local_host}:{local_port}:127.0.0.1:{target_port} -i {os.path.basename(fablib.get_default_slice_public_key_file())[:-4]} -F ssh_config {target_host}')\n",
    "print()\n",
    "print(\"After running the SSH command, open Open WebUI at http://localhost:8080. If prompted, create an account and start asking questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Slice\n",
    "\n",
    "Please delete your slice when you are done with your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ollama_node = fablib.get_slice(ollama_slice_name)\n",
    "#ollama_node.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

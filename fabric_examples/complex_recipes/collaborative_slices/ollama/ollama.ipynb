{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Slice, Launch ollama with LLM model\n",
    "\n",
    "This notebook provisions a slice on a single site, deploying one node equipped with a GPU and connected to a NIC_Basic via the FABNetv4 service. \n",
    "\n",
    "On this node, we install and configure Ollama to use the *deepseek-r1:7b* model and set up Open-WebUI on the VM. \n",
    "\n",
    "By establishing SSH tunnels, you can access Open-WebUI to submit queries through the web interface or interact with the LLM via the API. \n",
    "\n",
    "Additionally, nodes in other FABRIC slices connected to FABNetv4 can send queries to this LLM through the API over the FabNetv4 network. \n",
    "\n",
    "While this example utilizes FabNetv4, it can be adapted to work with the FabNetv6 service as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the FABlib Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipaddress import ip_address, IPv4Address, IPv6Address, IPv4Network, IPv6Network\n",
    "import ipaddress\n",
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "\n",
    "fablib = fablib_manager()\n",
    "\n",
    "fablib.show_config();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores_column_name = 'cores_available'\n",
    "ram_column_name = 'ram_available'\n",
    "disk_column_name = 'disk_available'\n",
    "\n",
    "core=16\n",
    "ram=32\n",
    "disk=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the Experiment Slice\n",
    "\n",
    "This section identifies a FABRIC site with an available GPU and sufficient CPU, RAM, and disk resources. Once a suitable site is found, a node is added with a GPU and a basic NIC, and it is connected to the FABNetv4 network to enable communication with other slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_slice_name = 'Ollama-deep-seek'\n",
    "\n",
    "ollama_node_name ='ollama_node'\n",
    "\n",
    "network_name='net1'\n",
    "nic_name = 'nic1'\n",
    "model_name = 'NIC_Basic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a Site  \n",
    "Choose a GPU model and search for a site that offers the specified GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choices include\n",
    "# GPU_RTX6000\n",
    "# GPU_TeslaT4\n",
    "# GPU_A30\n",
    "# GPU_A40\n",
    "GPU_CHOICE = 'GPU_A30' \n",
    "\n",
    "# don't edit - convert from GPU type to a resource column name\n",
    "# to use in filter lambda function below\n",
    "choice_to_column = {\n",
    "    \"GPU_RTX6000\": \"rtx6000_available\",\n",
    "    \"GPU_TeslaT4\": \"tesla_t4_available\",\n",
    "    \"GPU_A30\": \"a30_available\",\n",
    "    \"GPU_A40\": \"a40_available\"\n",
    "}\n",
    "\n",
    "column_name = choice_to_column.get(GPU_CHOICE, \"Unknown\")\n",
    "print(f'{column_name=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a site with at least one available GPU of the selected type\n",
    "site_override = None\n",
    "\n",
    "cores_column_name = 'cores_available'\n",
    "ram_column_name = 'ram_available'\n",
    "disk_column_name = 'disk_available'\n",
    "\n",
    "if site_override:\n",
    "    site1 = site_override\n",
    "else:\n",
    "    site1 = fablib.get_random_site(filter_function=lambda x: x[column_name] > 0 and \n",
    "                                   x[cores_column_name] > core and \n",
    "                                   x[ram_column_name] > ram and  \n",
    "                                   x[disk_column_name] > disk,\n",
    "                                  avoid = [\"GATECH\", \"GPN\"])\n",
    "    \n",
    "print(f'Preparing to create slice \"{ollama_slice_name}\" with node {ollama_node_name} in site {site1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the Slice  \n",
    "\n",
    "Users can specify alternative models such as:  \n",
    "\n",
    "`llama2-7b`, `mistral-7b`, `gemma-7b`, `deepseek-r1:67b`, `phi-2`, `gpt-neo-2.7b`  \n",
    "\n",
    "For more available models, visit: [Ollama Model Search](https://ollama.com/search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_llm_model = \"deepseek-r1:7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Slice\n",
    "ollama_slice = fablib.new_slice(name=ollama_slice_name)\n",
    "\n",
    "net1 = ollama_slice.add_l3network(name=network_name)\n",
    "\n",
    "ollama_node = ollama_slice.add_node(name=ollama_node_name, cores=core, ram=ram, \n",
    "                                    disk=disk, site=site1, image='default_ubuntu_22')\n",
    "\n",
    "ollama_node.add_component(model=GPU_CHOICE, name='gpu1')\n",
    "\n",
    "\n",
    "iface1 = ollama_node.add_component(model=model_name, name=nic_name).get_interfaces()[0]\n",
    "iface1.set_mode('auto')\n",
    "net1.add_interface(iface1)\n",
    "\n",
    "ollama_node.add_post_boot_upload_directory('ollama_tools','.')\n",
    "ollama_node.add_post_boot_upload_directory('node_tools','.')\n",
    "ollama_node.add_post_boot_execute('node_tools/enable_docker.sh {{ _self_.image }} ')\n",
    "ollama_node.add_post_boot_execute('node_tools/dependencies.sh {{ _self_.image }} ')\n",
    "ollama_node.add_post_boot_execute(f'cd ollama_tools && cp env.template .env && sed -i \"s/^MODEL_NAME=.*/MODEL_NAME={default_llm_model}/\" .env && docker compose up -d')\n",
    "\n",
    "ollama_slice.submit();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query LLM via API  \n",
    "\n",
    "This section demonstrates how to interact with the LLM using a Python API. We upload the `query.py` script to the `ollamanode` and execute it to send queries to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_slice=fablib.get_slice(ollama_slice_name)\n",
    "ollama_node = ollama_slice.get_node(ollama_node_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm Container Status  \n",
    "\n",
    "The containers may take a few minutes to start. Please verify that they are running before sending any queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(\"docker ps -a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(\"docker logs ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stdout, stderr = ollama_node.execute(\"docker logs open-webui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = ollama_node.execute(f'python3 ollama_tools/query.py --model {default_llm_model} --prompt \"Hello World\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Access to Ollama Node Across FABRIC  \n",
    "\n",
    "Configure the `ollamanode` to be accessible from any VM running across FABRIC on FabNetV4 by setting up the necessary routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_fabnet_network = ollama_slice.get_network(network_name)\n",
    "\n",
    "ollama_node.add_route(subnet=fablib.FABNETV4_SUBNET, \n",
    "                      next_hop=ollama_fabnet_network.get_gateway())\n",
    "\n",
    "ollama_node.config_routes()\n",
    "\n",
    "stdout, stderr = ollama_node.execute(\"sudo ip route list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the FabNet IP Address  \n",
    "Display the FabNet IP address of the Ollama node for sharing with other slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_fabnet_ip_addr = ollama_node.get_interface(network_name=network_name).get_ip_addr()\n",
    "\n",
    "print(f\"Ollama is accessible from other slices at: {ollama_fabnet_ip_addr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Slice\n",
    "\n",
    "Please delete your slice when you are done with your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ollama_node = fablib.get_slice(ollama_slice_name)\n",
    "ollama_node.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

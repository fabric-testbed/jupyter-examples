{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizations for High-bandwidth Wide-area Networking Experiments\n",
    "\n",
    "This example shows how to deploy a FABRIC slice across a wide-area connection and measure the bandwidth using iPerf3.\n",
    "\n",
    "### Background: iPerf3\n",
    "\n",
    "[iPerf](https://github.com/esnet/iperf) is a tool for measuring the maximum bandwidth achievable across IP networks. iPerf can be complicated to master and this notebook provides several tips and tricks for using iPerf on the high-bandwidth, high-latency dedicated links across the FABRIC testbed. The primary tips for using iPerf on FABRIC are to use the multithreaded version of iPerf3 and tune the host configuration.  \n",
    "\n",
    "The version of iPerf3 available from most Linux package managers (apt-get, yum, dnf, etc.) is limited by being single threaded. This can be confusing because all iPerf3 versions allow the `-P` option to set the number of parallel streams.  Although this option does change the number of streams, it typically uses a [single thread for all streams](https://fasterdata.es.net/performance-testing/network-troubleshooting-tools/iperf/multi-stream-iperf3/). Single-threaded iPerf3 will be limited to 20-30 Gbps. An updated multi-threaded version of [iPerf](https://github.com/esnet/iperf) is available from ESnet and can be used to test higher bandwidths. In order to simplify the use of multi-threaded iPerf3, FABRIC includes it in a Docker image that can be easily deployed in your experiment.  This notebook shows how to deploy and use this Docker container. \n",
    "\n",
    "Regardless of which version of iPerf3 you use, you will need to tune your host TCP/IP parameters in order to achieve higher bandwidths. ESnet's website contains a [discussion of host tuning](https://fasterdata.es.net/host-tuning/linux/) for network performance. This example provides a script with some initial tuning parameters. These tuning parameters are only suggestions and will not be optimal for all FABRIC networks. For maximum performance, you will need to optimize your configuration for your experiment.\n",
    "\n",
    "### Additional Features\n",
    "\n",
    "In addition to the new iPerf3 and tuning features, this notebook uses a few other advanced features.  You can learn about these features  in this notebook but additional information can be found at the links included here.\n",
    "\n",
    "- Automatic FABnet ([example](../../../fabric_examples/fablib_api/create_l3network_fabnet_ipv4_full_auto/create_l3network_fabnet_ipv4_full_auto.ipynb))\n",
    "- Templated Post boot tasks  ([example](../../../fabric_examples/fablib_api/post_boot_task_templates/post_boot_task_templates.ipynb))\n",
    "- Docker containers  ([example](../../../fabric_examples/fablib_api/docker_containers/docker_containers.ipynb))\n",
    "\n",
    "\n",
    "### FABlib API References\n",
    "\n",
    "- [fablib.get_random_sites](https://fabric-fablib.readthedocs.io/en/latest/fablib.html#fabrictestbed_extensions.fablib.fablib.FablibManager.get_random_sites)\n",
    "- [node.add_fabnet](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.add_fabnet)\n",
    "- [node.add_post_boot_upload_directory](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.add_post_boot_upload_directory)\n",
    "- [node.add_post_boot_execute](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.add_post_boot_execute)\n",
    "- [node.numa_tune](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.numa_tune)\n",
    "- [node.pin_cpu](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.pin_cpu)\n",
    "- [node.os_reboot](https://fabric-fablib.readthedocs.io/en/latest/node.html#fabrictestbed_extensions.fablib.node.Node.os_reboot)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the FABlib Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "\n",
    "fablib = fablib_manager()\n",
    "\n",
    "fablib.show_config();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the Experiment Slice\n",
    "\n",
    "The simplest slice that can be used for wide-area iPerf3 performance tests is a pair of nodes on two different FABRIC sites connected to the FABnet network.  The following cells build an appropriate slice including post boot configuration of Docker and host network tuning. \n",
    "\n",
    "The slice deployed here uses basic NICs and VMs with modest amounts of compute cores and memory. Achieving the highest bandwidths possible will require additional infrastructure and tuning including VMs with larger capacities, dedicated network cards, and pinning cores/memory to NUMA domains. \n",
    "\n",
    "<img src=\"./figs/iperf3_dumbell.png\" width=\"70%\"><br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Two Random Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slice_name = 'iPerf3'\n",
    "[site1, site2] = fablib.get_random_sites(count=2)\n",
    "#site1='NEWY'\n",
    "#site2='SALT'\n",
    "\n",
    "print(f\"Sites: {site1}, {site2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create Slice\n",
    "slice = fablib.new_slice(name=slice_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Nodes and Network\n",
    "\n",
    "Add two nodes to the slice. Place one node on each of the sites.  \n",
    "\n",
    "Add a fully automatic FABnetv4 network to each node using the `add_fabnet` method.  This method will create a FABnet network on each site, add a basic NIC to each node, and automatically configure IP addresses and routes.\n",
    "\n",
    "Note the use of the `docker_rocky_8` image. This image comes with Docker pre-installed enabling fast/simple deployment of Docker containers. \n",
    "\n",
    "The nodes are configured with only 4 cores and 8 GB of RAM. Higher bandwidth will require larger VMs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node1 = slice.add_node(name='Node1', cores=8, ram=16, disk=100, site=site1) #, image='docker_ubuntu_22')\n",
    "node1.add_fabnet()\n",
    "\n",
    "node2 = slice.add_node(name='Node2', cores=8, ram=16, disk=100, site=site2) #, image='docker_ubuntu_22')\n",
    "node2.add_fabnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Post Boot Configuration Tasks\n",
    "\n",
    "This example includes a directory containing a couple scripts that should be used to configure the nodes. The following post boot tasks will execute after the nodes are booted.  \n",
    "\n",
    "The tasks include:\n",
    "\n",
    "- Upload node tools: Copy the `node_tools` directory to each node. This directory contains the custom configuration scripts. \n",
    "- Execute `host_tune.sh`: Execute the script that tunes the host for high-bandwidth, high-latency data transfers. Feel free to customize this script for your specific experiment.\n",
    "- Execute `enable_docker.sh`: This script enables the pre-installed Docker services. The image argument is an example of using templated post boot tasks. \n",
    "- Execute Docker pull to get required Docker container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node1.add_post_boot_upload_directory('node_tools','.')\n",
    "node1.add_post_boot_execute('sudo node_tools/host_tune.sh')\n",
    "node1.add_post_boot_execute('node_tools/enable_docker.sh {{ _self_.image }} ')\n",
    "node1.add_post_boot_execute('docker pull fabrictestbed/slice-vm-rocky8-multitool:0.0.2 ')\n",
    "#node1.add_post_boot_execute('chmod +x node_tools/escp_install.sh ')\n",
    "#node1.add_post_boot_execute('node_tools/escp_install.sh ')\n",
    "\n",
    "\n",
    "node2.add_post_boot_upload_directory('node_tools','.')\n",
    "node2.add_post_boot_execute('sudo node_tools/host_tune.sh')\n",
    "node2.add_post_boot_execute('node_tools/enable_docker.sh {{ _self_.image }} ')\n",
    "node2.add_post_boot_execute('docker pull fabrictestbed/slice-vm-rocky8-multitool:0.0.2 ')\n",
    "#node2.add_post_boot_execute('chmod +x node_tools/escp_install.sh ')\n",
    "#node2.add_post_boot_execute('node_tools/escp_install.sh ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the Slice\n",
    "\n",
    "The slice request is complete and you can now submit the request. Notice the `post_boot_config` step. During this step, the FABnet network and post boot tasks that were added in the previous cells are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Submit Slice Request\n",
    "slice.submit();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run iPerf3\n",
    "\n",
    "Running iPerf3 is simple using the supplied Docker image.  \n",
    "\n",
    "This cell gets both `node1` and `node2` then gets the target IP address `node1` uses on the FABnet data plane.  It then starts an iPerf3 server in a Docker container on `node1` and an iPerf3 client in a Docker container on `node2`.  The server is configured with the `-1` parameter instructing it to exit after one iPerf3 session. The client is configured to connect to the server's data plane IP.\n",
    "\n",
    "This cell can be re-run many times.  You may wish to modify the iPerf3 parameters that are passed to the client Docker container to see how they affect the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slice = fablib.get_slice(slice_name)\n",
    "\n",
    "node1 = slice.get_node(name='Node1')        \n",
    "node2 = slice.get_node(name='Node2')           \n",
    "\n",
    "node1_addr = node1.get_interface(network_name=f'FABNET_IPv4_{node1.get_site()}').get_ip_addr()\n",
    "\n",
    "stdout1, stderr1 = node1.execute(\"docker run -d --rm \"\n",
    "                                \"--network host \"\n",
    "                                \"fabrictestbed/slice-vm-rocky8-multitool:0.0.2 \"\n",
    "                                \"iperf3 -s -1\"\n",
    "                                , quiet=True, output_file=f\"{node1.get_name()}.log\");\n",
    "\n",
    "stdout2, stderr2 = node2.execute(\"docker run --rm \"\n",
    "                                \"--network host \"\n",
    "                                \"fabrictestbed/slice-vm-rocky8-multitool:0.0.2 \"\n",
    "                                f\"iperf3 -c {node1_addr} -P 4 -t 30 -i 10 -O 10\"\n",
    "                                , quiet=False, output_file=f\"{node2.get_name()}.log\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) NUMA Tuning\n",
    "\n",
    "Higher bandwidths can only be achieved with larger VMs that have their CPU cores pinned to the same NUMA domain as the PCI bus that handles their NIC.  The following cell will attempt to pin all of the CPU cores and memory used by your VMs to the same NUMA domain as the NIC that they are using.  \n",
    "\n",
    "It is important to note that core and memory pinning grants a VM exclusive access to specific resources in the host machines.  NUMA tuning may not be possible because cores and memory cannot be pinned to resources that are already allocated to other VMs owned by other users. If NUMA pinning is not possible, you will see an appropriate error stating that the resources are not currently available.\n",
    "\n",
    "Note that if you are currently attending a tutorial, there is an increased likelihood that the resources you are requesting have already been allocated to one of your fellow attendees.\n",
    "\n",
    "### Pin CPUs and Memory\n",
    "\n",
    "The `pin_cpu` method attempts to pin all of your CPU cores to the same NUMA domain as the PCI component that is your NIC. The `numa_tune` method attempts to pin all of your VM's memory to the same NUMA domain as the PCI component that is your NIC.  After pinning, you must reboot each node.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = fablib.get_slice(slice_name)\n",
    "\n",
    "for node in slice.get_nodes():\n",
    "\n",
    "    print(f'----- Pinning vCPUs for node {node.get_name()} ------')\n",
    "\n",
    "    try:\n",
    "       # Pin all vCPUs for VM to same Numa node as the component\n",
    "        node.pin_cpu(component_name=f'FABNET_IPv4_{node.get_site()}_nic')\n",
    "        \n",
    "        # User can also pass in the range of the vCPUs to be pinned \n",
    "        #node.pin_cpu(component_name=nic_name, cpu_range_to_pin=\"0-3\")\n",
    "        \n",
    "        # Pin memmory for VM to same Numa node as the components\n",
    "        node.numa_tune()\n",
    "        \n",
    "        # Reboot the VM\n",
    "        node.os_reboot()\n",
    "    except Exception as e:\n",
    "        print(f'{e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait and Reconfigure\n",
    "\n",
    "Use the `wait_ssh` method to block and wait for your nodes to reboot and be accessible using ssh.\n",
    "\n",
    "After rebooting, you will need to reconfigure the IP addresses of the interfaces and restart the Docker services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = fablib.get_slice(slice_name)\n",
    "\n",
    "# Wait for the SSH Connectivity to be back\n",
    "slice.wait_ssh()\n",
    "\n",
    "print(\"All nodes are back up!\")\n",
    "\n",
    "# Reconfiguring the Network\n",
    "for node in slice.get_nodes():\n",
    "    print(f'Reconfiguring node {node.get_name()}')\n",
    "    node.config()\n",
    "\n",
    "    stdout1, stderr1 = node.execute(f\"sudo systemctl start docker\", \n",
    "                                    output_file=f'{node.get_name()}.log', quiet=True)    \n",
    "    stdout1, stderr1 = node.execute(f\"sudo ./node_tools/host_tune.sh\", \n",
    "                                    output_file=f'{node.get_name()}.log', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Run iPerf3\n",
    "\n",
    "Re-run iPerf3 exactly as you ran it before.  \n",
    "\n",
    "Did you achieve higher performance? Why or why not?\n",
    "\n",
    "Can you configure your slice to achieve performance approaching the 100 Gbps limit imposed by these FABRIC NICs?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slice = fablib.get_slice(slice_name)\n",
    "\n",
    "node1 = slice.get_node(name='Node1')        \n",
    "node2 = slice.get_node(name='Node2')           \n",
    "\n",
    "node1_addr = node1.get_interface(network_name=f'FABNET_IPv4_{node1.get_site()}').get_ip_addr()\n",
    "\n",
    "stdout1, stderr1 = node1.execute(\"docker run -d --rm \"\n",
    "                                \"--network host \"\n",
    "                                \"fabrictestbed/slice-vm-rocky8-multitool:0.0.2 \"\n",
    "                                \"iperf3 -s -1\"\n",
    "                                , quiet=True, output_file=f\"{node1.get_name()}.log\");\n",
    "\n",
    "stdout2, stderr2 = node2.execute(\"docker run --rm \"\n",
    "                                \"--network host \"\n",
    "                                \"fabrictestbed/slice-vm-rocky8-multitool:0.0.2 \"\n",
    "                                f\"iperf3 -c {node1_addr} -P 8 -t 30 -i 10 -O 10\"\n",
    "                                , quiet=False, output_file=f\"{node2.get_name()}.log\");\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Slice\n",
    "\n",
    "Please delete your slice when you are done with your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slice = fablib.get_slice(slice_name)\n",
    "slice.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
